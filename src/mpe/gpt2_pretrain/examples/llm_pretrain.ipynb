{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, GPT2Model, GPT2LMHeadModel\n",
    "from transformers.generation.configuration_utils import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "n_embed = 768\n",
    "batch_size = 16\n",
    "\n",
    "# RL training params\n",
    "n_episodes = 500\n",
    "n_landmarks = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# IO dimensions\n",
    "speaker_in = 3\n",
    "speaker_out = 5\n",
    "listener_in = n_landmarks*2\n",
    "listener_out = 2\n",
    "\n",
    "# speaker generation params\n",
    "num_beams = 5\n",
    "early_stopping = True\n",
    "no_repeat_ngram_size = 1\n",
    "do_sample = True\n",
    "temperature = 0.9\n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "max_length = speaker_out\n",
    "\n",
    "\n",
    "# landamrk colors\n",
    "landmarks_c = sns.color_palette(n_colors=n_landmarks)\n",
    "landmarks_c = torch.tensor(landmarks_c).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speaker(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    # init GPT2\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    self.transformer = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # config file when generating text\n",
    "    self.generation_config = GenerationConfig.from_model_config(self.transformer.config)\n",
    "\n",
    "    # freeze GPT2 params\n",
    "    for param in self.transformer.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # custom embedding layer for obs\n",
    "    self.embed_obs = nn.Linear(speaker_in, n_embed)\n",
    "\n",
    "\n",
    "  def forward(self, obs: torch.Tensor):\n",
    "    ''' \n",
    "    The observation of the speaker is the color of the target landmark of the listener. \n",
    "    Given the color, the speaker returns a message to the listener.\n",
    "    '''\n",
    "\n",
    "    # dims\n",
    "    batch_size = obs.shape[0]\n",
    "\n",
    "    # compute embeddings of observation (the color)\n",
    "    obs_embeddings = self.embed_obs(obs)\n",
    "    attention_mask = torch.ones((batch_size, obs_embeddings.shape[1]))\n",
    "    # generate text (sampling based)\n",
    "    msg = self.transformer.generate(\n",
    "                          inputs_embeds=obs_embeddings,\n",
    "                          attention_mask=attention_mask, \n",
    "                          generation_config=self.generation_config, \n",
    "                          num_beams=num_beams,\n",
    "                          early_stopping=early_stopping,\n",
    "                          no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                          do_sample=do_sample,\n",
    "                          temperature=temperature,\n",
    "                          top_k=top_k,\n",
    "                          top_p=top_p,\n",
    "                          max_length=max_length,\n",
    "                          pad_token_id=self.tokenizer.eos_token_id)\n",
    "\n",
    "    return msg\n",
    "\n",
    "\n",
    "class Listener(nn.Module):\n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()\n",
    "\n",
    "    # init GPT2\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    self.transformer = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # freeze GPT2 params\n",
    "    for param in self.transformer.parameters():\n",
    "      param.requires_grad = False\n",
    "\n",
    "    # GPT2 word embedding layer\n",
    "    self.wte = nn.Embedding.from_pretrained(self.transformer.wte.weight)\n",
    "\n",
    "    # custom embedding layer for processing input obs\n",
    "    self.embed_obs = nn.Linear(listener_in, n_embed)\n",
    "\n",
    "    # custom prediction layer for computing the action\n",
    "    self.predict_action = nn.Linear(n_embed*(1+speaker_out), listener_out)\n",
    "\n",
    "  def forward(self, obs:torch.Tensor, msg:torch.Tensor):\n",
    "    '''\n",
    "    obs: position of landmarks\n",
    "    msg: message from speaker indicating which landmark to go to\n",
    "    '''\n",
    "\n",
    "    # compute and stack embeddings\n",
    "    word_embeddings = self.wte(msg)\n",
    "    obs_embeddings = self.embed_obs(obs).unsqueeze(1)\n",
    "    stacked_inputs = torch.cat((word_embeddings, obs_embeddings), 1)\n",
    "\n",
    "    # compute transformer hidden states\n",
    "    hidden_state = self.transformer(inputs_embeds=stacked_inputs)['last_hidden_state']\n",
    "\n",
    "    # reshape hidden states and pass through prediction layer\n",
    "    x = hidden_state.view((batch_size, -1))\n",
    "    action = self.predict_action(x)\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models\n",
    "speaker = Speaker().to(device)\n",
    "listener = Listener().to(device)\n",
    "\n",
    "# init optimizer\n",
    "optimizer = torch.optim.Adam(params=list(speaker.parameters())+list(listener.parameters()), lr=1e-3)\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in tqdm(range(n_episodes)):\n",
    "  # random landmark position\n",
    "  landmarks_p = (torch.rand((batch_size, 2*n_landmarks)) - 0.5) * 2\n",
    "  # sample target landmark indices\n",
    "  ids = torch.randint(n_landmarks, (batch_size,))\n",
    "  # speaker input\n",
    "  goal_landmarks = landmarks_c.repeat(batch_size, 1, 1)[ids]\n",
    "  # messages from speaker\n",
    "  msg = speaker(goal_landmarks)\n",
    "  # compute action from lisener\n",
    "  action = listener(landmarks_p, msg)\n",
    "  # ground truth target actions\n",
    "  target = torch.cat([landmarks_p[i, ix*2:(ix+1)*2].unsqueeze(0) for i, ix in enumerate(ids)], 0)\n",
    "  # backprop\n",
    "  optimizer.zero_grad()\n",
    "  loss = lossfun(action, target)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if i%10==0:\n",
    "    #print(f\"loss = {loss.item()}\")\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "plt.plot(loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('maddpg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b948fd4f360e982bb8a0c7ddab2f8dd577e6a922eb0395cc98ad0ffb372baf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
